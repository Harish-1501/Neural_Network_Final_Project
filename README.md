# YouTube Comment Toxicity Classifier (Streamlit + Keras BiLSTM)

A simple web app for classifying YouTube comments as **toxic / nonâ€‘toxic** using a trained **BiLSTM** model.  
Built with **Streamlit**, loads a saved model (`best.keras`) and tokenizer (`tokenizer.json` or `tokenizer.zip`), and supports both **single-text** and **batch CSV** predictions.

---

** Streamlit Link : **
https://harish-1501-neural-network-final-project-app-3injs6.streamlit.app/


## ğŸ§ª How to Use

### Single-text prediction
1. Paste/enter your text in the left panel.
2. Click **Predict**.
3. Youâ€™ll see:
   - **probability** (modelâ€™s sigmoid output),
   - **label** determined by your **threshold** (default 0.50; adjustable in sidebar).

### Batch prediction (CSV)
1. Upload a CSV with at least one **text column**.
2. Pick the column from the dropdown.
3. Click **Run batch predictions** to get a preview and a **Download** button.

---

## âš™ï¸ Tuning the Decision Threshold

If you previously computed a best threshold (for example, maximizing F1 on validation), enter that value in the sidebar.  
- `probability >= threshold` â†’ **positive** (default label name â€œpositiveâ€)  
- otherwise â†’ **negative**

You can also rename the positive/negative labels in the sidebar settings.

---

## âœ¨ Features

- **Single-text prediction** with probability bar and label.
- **Batch CSV prediction** with preview + downloadable results.
- **Exact training-time tokenizer** (JSON or inside ZIP) applied at inference.
- **Precise threshold control** (number input), with optional **invert probability (1 âˆ’ p)** toggle.
- **Keras 3â€“aware loading** (uses `keras.saving.load_model`, falls back to `tf.keras` if needed).
- **Runtime debug panel** showing raw sigmoid and used probability for transparency.

---

## ğŸ§  Model

- **Architecture:** Embedding (vocab=20,000, dim=128, `mask_zero=True`) â†’ SpatialDropout1D(0.2) â†’ **Bidirectional LSTM(64, return_sequences=True)** â†’ GlobalMaxPooling1D â†’ Dense(64, ReLU) â†’ Dropout(0.3) â†’ Dense(1, **sigmoid**).
- **Input:** int32 token IDs, **shape `[batch, 512]`**, padded/truncated with `0`s (post).
- **Output:** float32 **sigmoid** `P(class=1)` (probability of the trained positive class).
- **Validation/held-out metrics (provided by user):**
  - **best_threshold:** `0.0487249419`
  - **best_val_f1:** `0.9514563107`
  - **ROC AUC:** `0.8839164895`
  - **PR AUC:** `0.9847274865`
  - **Support (test):** pos=`739`, neg=`84`

> âš ï¸ Be sure what **class 1** meant in your training (e.g., *toxic*). If your UI calls the opposite class â€œpositiveâ€, either rename labels or use the **Invert probability (1 âˆ’ p)** toggle.

---

## ğŸ—‚ï¸ Repository Structure

```
.
â”œâ”€â”€ app.py                  # Streamlit app
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best.keras          # Trained Keras model (Keras 3 format)
â”‚   â””â”€â”€ tokenizer.json      # Or tokenizer.zip containing tokenizer.json
â”œâ”€â”€ data/                   # (optional) datasets, samples, exports
â””â”€â”€ README.md               # This file
```

> Large model files? Use **Git LFS** or a startup download step.

---

## ğŸ”¢ Decision Thresholds

- If **class 1 = toxic** and your UI â€œPositive labelâ€ = **toxic** â†’ set threshold to **`0.0487249419`**.
- If you want â€œPositive labelâ€ = **non-toxic** instead, turn on **Invert probability (1 âˆ’ p)** and use the **complement threshold**: `1 âˆ’ 0.0487249419 = 0.9512750581`.

You can change label names and threshold in the appâ€™s **sidebar**.

---

## ğŸ’¾ Data (YouTube IDs â†’ Comments)

This projectâ€™s comments were extracted **from YouTube video IDs** using the **YouTube Data API**. A typical pipeline:

1. **Prepare video IDs**: a CSV or text file of `video_id`s.
2. **Fetch comments** with the `commentThreads.list` endpoint (and optionally `comments.list` for replies).
3. **Store** raw fields such as:
   - `video_id`, `comment_id`, `textOriginal`, `likeCount`, `publishedAt`,
   - `authorDisplayName`, `parentId` (if reply), `replyCount` (thread level).
4. **Clean text** (strip markup, handle emojis, normalize whitespace).
5. **Split** into train/val/test and apply **your tokenizer** for model training.

**Environment variables** commonly used:
```
YOUTUBE_API_KEY=<your_api_key>
```

**Notes & compliance**:
- Respect YouTubeâ€™s **Terms of Service**, data retention, and user privacy.
- Observe **API quota limits**; paginate on `nextPageToken`.
- Attribute responsibly and avoid redistributing personal data.

---

## ğŸ§° Setup

### 1) Python Environment
We recommend Python **3.9â€“3.11**.

```bash
python -m venv .venv
# Windows: .venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate
pip install -r requirements.txt
```

`requirements.txt` pins modern **TensorFlow (â‰¥2.17)** and **Keras (â‰¥3.3)** so models saved with **Keras 3** load correctly.

### 2) Files Required
Place your artifacts as follows:

```
models/
  best.keras
  tokenizer.json        # or tokenizer.zip containing tokenizer.json
```

---

## â–¶ï¸ Run Locally

```bash
streamlit run app.py
```
Open the URL Streamlit prints (usually http://localhost:8501).

---

## â˜ï¸ Deploy (Streamlit Community Cloud)

1. Push repo to GitHub with the structure above.
2. Go to https://share.streamlit.io and connect your repo.
3. Set **Main file path** to `app.py` and deploy.

> For big model files, use **Git LFS** or load from cloud storage on startup.

---

## ğŸ“„ License

MIT (or your preferred license).

---

## ğŸ™Œ Acknowledgments

- YouTube Data API for data access.
- Keras / TensorFlow and Streamlit for the ML and UI stack
- Neural Network Course and our guide Mohammad Saiful Islam (https://www.linkedin.com/in/mohammadsaifulislam/)